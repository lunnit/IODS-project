# Assignment 4: clustering and classification 

```{r}
#libraries
library(tidyr)
library (MASS)
library(corrplot)
library(ggplot2)
```

## A brief description of the data set

This time we are using data set called “Boston”. This data set concerns housing and housing values in Boston’s suburbs. The data has 506 rows, which in this case mean towns or districts and 14 columns e.g., variables.  The variables describe factors which affect the town's housing values. These include factors like crime rate per capita, the average number of rooms per dwelling, property tax rate, pupil-teacher ratio, distances to a river and closeness to highways.More information about the data can be found here: https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html

In this exercise, the main focus will be on the crime rate variable. 

```{r}

# loading the data
data("Boston")

# exploring the dataset

str(Boston)
dim(Boston)
```

## Visual examination of the data

The next step is again to visualise the data. 

The pairs -plot shows us that the most of variables do not have a nice correlation between them. For example, “chas” is a binary dummy variable describing closeness to the river. Additionally. there are several other rather funny-looking distributions. For instance. variables “rad” and “black” looks a bit like bimodal distribution. Fitting a linear model seem rather difficult according to these plots. 

The next plot highlights correlations between the variables in the data. Our outcome variable of the week, crime rate, seems to have a moderately high positive correlation with property tax rate /tax) and index of accessibility to radial highways (rad).  The strongest positive correlation can be found between rad and tax. The strongest negative correlation is harder to pick out. There are three equally strong competitors: 1. lower status of the population (lstat) and median value of the owner-occupied homes (medv), 2. mean of distances to five Boston employment centres (dis) and proportion of owner-occupied units built prior to 1940 (age)  and 3. mean of distances to five Boston employment centres (dis) and nitrogen oxides concentration (nix). 


```{r}

summary(Boston)


pairs(Boston)

cor_matrix <- cor(Boston) 

cor_matrix %>%
  round(2)

#corrplot(cor_matrix, method="circle",)

corrplot(cor_matrix, method="circle", type ="upper",cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

```


## Standardizing the dataset 

Standardizing the data set sets the mean of all variables to zero. After this procedure, the variables are in a more comparable form. In other words, all of the variables are on the same scale after standardizing the data set, so to speak. 
 

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

summary((boston_scaled))

boston_scaled <- as.data.frame(boston_scaled)

bins <- quantile(boston_scaled$crim)
bins

# creating a categorical variable 'crime'
v<- c("low", "med_low", "med_high", "high")
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=v)

table(crime)

# removing the original
boston_scaled <- dplyr::select(boston_scaled, -crim)

# adding the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)



#dividing the dataset

n <- nrow(boston_scaled)

# choosing randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# creating a train set
train <- boston_scaled[ind,]

# creating a test set 
test <- boston_scaled[-ind,]

# saving the correct classes from test data
correct_classes <- test$crime

# removing the crime variable from test data
test <- dplyr::select(test, -crime)

```


##  Linear discriminant analysis (LDA)

Linear discriminant analysis (LDA) is a dimensionality reduction technique. it is commonly used for modelling differences in groups. In this case, it is used to examine crime rates in Boston. 

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)

```

## Predictive power of the model

The model deems to predict the cases correctly quite well. there are eight wrong predictions in the low crime rate category: six of them are predicted in the medium-low and two in the medium-high category. Similarly, there are nine mispredictions in the medium-low category: five in the low and 4 in the medium-high category.  Medium-high predictions seem to be rather inaccurate: in this case, there are predicted values in all of the other categories as well. Lastly, the high crime rate seems to be predicted the most accurately as there is not a single inaccurately predicted case.  

Anyway, depending on the crime rate, the method classified cases correctly from 53 % to 96 % of the cases. 

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

round(prop.table(table(correct = correct_classes, predicted = lda.pred$class), margin = 1),2)

```

## K-means clustering

K-means clustering is a technique to group data into a fixed number of clusters. This is done with a similarity: data points are clustered to the cluster nearest cluster while keeping the centre of the clusters as small as possible. 

```{r}
data("Boston")
Boston <- scale(Boston)

# euclidean distance matrix
dist_eu <- dist(Boston)

summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(Boston, method="manhattan")

summary(dist_man)


#k-means

# k-means clustering
set.seed(13)
km <- kmeans(Boston, centers =3)

#km$cluster

# plotting the Boston dataset with clusters
pairs(Boston, col = km$cluster )

#pairs(Boston[6:10], col = km$cluster )


```

The analysis above is done with three clusters and I do not have any justifications for the number of clusters. However, there are means to find out the optimal number of clusters. This is illustrated in the line chart where the optimal amount of clusters is seen as a sudden drop of the line in the chart.  Here it seems to be around two. 

Therefore we are running the k-means again with just two centres. 
As for interpreting the results, it seems that the characteristics of towns form two “archetypes” which have bit different features. For example, tax rates seem to be different in two clusters.

```{r}

set.seed(123)

# determining the number of clusters
k_max <- 10

# calculating the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualizing the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <- kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

```


