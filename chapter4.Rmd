# Assignment 4: clustering and classification 

```{r}
#libraries
library(tidyr)
library (MASS)
library(corrplot)
library(ggplot2)
```

## A brief description of the data set

This time we are using data set called “Boston”. This data set concerns housing and housing values in Boston’s suburbs. The data has 506 rows, which in this case mean towns or districts and 14 columns e.g., variables.  The variables describe factors which affect the town's housing values. These include factors like crime rate per capita, the average number of rooms per dwelling, property tax rate, pupil-teacher ratio, distances to a river and closeness to highways.More information about the data can be found here: https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html

In this exercise, the main focus will be on the crime rate variable. 

```{r}

# loading the data
data("Boston")

# exploring the dataset

str(Boston)
dim(Boston)
```

## Visual examination of the data

The next step is again to visualise the data. 

The pairs -plot shows us that the most of variables do not have a nice correlation between them. For example, “chas” is a binary dummy variable describing closeness to the river. Additionally. there are several other rather funny-looking distributions. For instance. variables “rad” and “black” looks a bit like bimodal distribution. Fitting a linear model seem rather difficult according to these plots. 

The next plot highlights correlations between the variables in the data. Our outcome variable of the week, crime rate, seems to have a moderately high positive correlation with property tax rate /tax) and index of accessibility to radial highways (rad).  The strongest positive correlation can be found between rad and tax. The strongest negative correlation is harder to pick out. There are three equally strong competitors: 1. lower status of the population (lstat) and median value of the owner-occupied homes (medv), 2. mean of distances to five Boston employment centres (dis) and proportion of owner-occupied units built prior to 1940 (age)  and 3. mean of distances to five Boston employment centres (dis) and nitrogen oxides concentration (nix). 


```{r}

summary(Boston)


pairs(Boston)

cor_matrix <- cor(Boston) 

cor_matrix %>%
  round(2)

#corrplot(cor_matrix, method="circle",)

corrplot(cor_matrix, method="circle", type ="upper",cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

```


## Standardizing the dataset 

Standardizing the data set sets the mean of all variables to zero. After this procedure, the variables are in a more comparable form. In other words, all of the variables are on the same scale after standardizing the data set, so to speak. 
 

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

summary((boston_scaled))

boston_scaled <- as.data.frame(boston_scaled)

bins <- quantile(boston_scaled$crim)
bins

# creating a categorical variable 'crime'
v<- c("low", "med_low", "med_high", "high")
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=v)

table(crime)

# removing the original
boston_scaled <- dplyr::select(boston_scaled, -crim)

# adding the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)



#dividing the dataset

n <- nrow(boston_scaled)

# choosing randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# creating a train set
train <- boston_scaled[ind,]

# creating a test set 
test <- boston_scaled[-ind,]

# saving the correct classes from test data
correct_classes <- test$crime

# removing the crime variable from test data
test <- dplyr::select(test, -crime)

```


##  Linear discriminant analysis (LDA)


```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)

```

#6

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```

#7

```{r}
data("Boston")
Boston <- scale(Boston)
#summary(Boston)


# euclidean distance matrix
dist_eu <- dist(Boston)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(Boston, method="manhattan")

# look at the summary of the distances
#summary(dist_eu)
summary(dist_man)



#k-means

# k-means clustering
set.seed(13)
km <- kmeans(Boston, centers =3)

#km$cluster

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster )

#pairs(Boston[6:10], col = km$cluster )



set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <- kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)


```


