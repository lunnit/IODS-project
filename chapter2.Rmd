# Assigment 2: linear regression


*Describe the work you have done this week and summarize your learning.*

- Describe your work and results clearly. 
- Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods.
- Assume the reader has no previous knowledge of your data or the more advanced methods you are using.

```{r}
date()
```

```{r}
#installing libraries:
library(ggplot2)
library(GGally)
library(car)

#getting the data
lrn14 <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/learning2014.txt", sep=",", header=TRUE)

```


## Background info on the data set

The data in use is a subset of the data collected by ASSIST 2014 -project. The subset consists of 66 observations or participants and 7 variables: gender, age, attitude, points, deep approach, surface approach, and strategic approach. Participants, who did not get any points in the exam are excluded from this subset. So, N of the data was originally 183 and 17 got excluded. 

As their names indicate, most of the variables concern learning and study skills. Points refers to points gained in an exam (presumably an exam concerning statistics). Attitude is an attitude toward statistics. 

The last three variables are sum variables that represent different approaches to studying and preparing for the exam. The sum variables are scaled back to their original scale (1-5).

In the next section, we take a closer look at the data.


```{r}
#exploring the data
dim(lrn14)
str(lrn14)
```

## Closer look at the data

This section presents firstly individual summaries of the variables and secondly two scatterplot matrices, which provide an easy way to eyeball associations between different variables. 

```{r}
table(lrn14$gender)
summary(lrn14$age)
summary (lrn14$attitude)
summary (lrn14$deep)
summary (lrn14$stra)
summary (lrn14$surf)
summary (lrn14$points)


```

The gender distribution of the data is rather unbalanced: there are 110 women and 56 men. Similarly, the data skews towards younger people: the median age is 22.0 and the mean is 25.51. The age range is 17.00--55.00. However, this is not surprising as the data is probably collected among university students. 


Correlation matrices are a good way to get an impression of the data. In this case, there are two matrices presented. In the second one, the data is divided by gender. However, it is important to remember that, correlations cannot tell us which variable precedes the other. They simply present associations between variables. 


```{r}
p1 <- ggpairs(lrn14, mapping = aes(), lower = list(combo = wrap("facethist", bins = 20)))

p2 <- ggpairs(lrn14, mapping = aes(col=gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p1
p2

```


Overall correlations in the data set are weak and only a handful of them are statistically significant. The strongest correlation is found between attitude and points. The correlation is  0.437 with p>0.001. The correlation is a bit stronger among males (0.451, p>0.001) than females (0.422, p>0.001). Basically, this means that people who have a more positive attitude toward statistics are the same who gained more points in the exam and vice versa.

Additionally, attitude correlates with the surface approach to studying, and this time the correlation is negative: -1.76 with p>0.05. However, gender-wise, the correlation is statistically significant only among men (-0.374, p>0.01).  A negative correlation simply means that people with a more positive attitude are the same that rarely apply the surface approach and vice versa.

The surface approach also correlates with other approach types. The correlation between the surface approach and the deep approach is -0.324, p>0.01. Also, this correlation is statistically significant only among men (-0.622, p>0.01). Furthermore, this correlation illustrates rather clearly the need for a visual examination of the data. The scatterplot shows us that the correlation might be due to outliers: there are two or three data points in the left upper corner which probably are to be blamed at least to some extent for the correlation.  

In addition to the deep approach, the surface approach also correlates with the strategic approach. The correlation is -.161 with p>0.05. To summarise, the surface approach correlates negatively with two other approaches and with attitude. This can imply multicollinearity, which then may cause problems when using these variables as predictors. 


## Regression model

This section presents (linear) regression models that aim to predict points gained in the exam. The modelling applies a simplified version of the backward elimination method to find the best possible model. In layman’s terms, the initial model includes all variables that are interesting (according to previous studies if this was a real study) and then variables are excluded from the model if they are not associated with the outcome. The selection criterion used in an actual backward elimination is a method called  Akaike’s information criterion (AIC). However, it is a bit advanced for this practice assignment and therefore statistical significance is used as a replacement for AIC.

Explanatory variables of the first model are attitude, the surface approach, and the strategic approach. Variables were chosen because they had the highest correlations with the outcome variable. Multicollinearity of the variables is also tested. 

```{r}
model1 <- lm(points~ attitude + stra + surf, data=lrn14 )
summary(model1)
vif(model1)

```
As mentioned, the first model has three explanatory variables: attitude the surface approach and the strategic approach. Firstly, let's take a look at the F-value, which tells us if the chosen variables can be used to predict the outcome. The test would be non-significant if the variables did not explain any variation in the outcome variable. The f-value of the first model is 14.13 and its p-value is 3.156e-08, which is highly statistically significant. Therefore, the model passes the first test. 

Another test or measure to look at is the multiple R-squared. In this case, it is 0.2074. This means that the variables included in the model account for circa 20 % of the variation in the outcome variable. That is to say, attitude, the strategic approach and the surface approach together explain circa 20 % of the variation in the exam points.

Multicollinearity should not be a concern in this model as VIF-values are all below 2. 

However, only attitude has a statistically significant coefficient. The coefficient’s value is circa 3.40 and its p-value is 1.93e-08. Interpretation of the coefficient is the following: if attitude is increased by one point, points gained in the exam increases also by 3.4 points if everything else is kept constant.


As instructed, the next model includes only statistically significant variables. In this case, that means using only attitude to predict points. 

```{r}
model <- lm(points~ attitude, data=lrn14 )
summary(model)
```
The F-value of the second model is 38.61 with a p-value of 4.119e-09. This is still statistically highly significant. 

The multiple R-squared for the second model is 0.1906. A slight decline in this value is to be expected as the model has fewer explanatory variables. However, the decline is slight enough not to be worried about: the model still explains almost 20 % of the outcome variable’s variation. 

The coefficient for attitude is circa 3.5 and it's still highly statistically significant.  So, according to the second model, a single score increase in attitude increases points by 3.5 points. Thus, the attitude seems to predict points gained in the exam quite well. However, the model should still be put through diagnostics. 


## Diagnostics

Linear regression makes several assumptions. The most crucial ones are:  
1.	Linear relationship between predictors and outcome;
2.	Independence of residuals;
3.	Normal distribution of residuals;
4.	Equal variance of residuals. 

This section examines how the second model follows these assumptions. 


```{r}
p1 <- ggplot(lrn14, aes(x = attitude, y = points)) + geom_point()
p2 <- p1 + geom_smooth(method = "lm")
p2

par(mfrow = c(2,2))
x<-c(1,2,5)
plot(model, which =x)

```


The first assumption is easy to check: plotting a simple scatterplot reveals if the relationship between the predictor(s) and the outcome is linear. The first plot portrays this relationship, and it does not give any reason to worry about the model as the data describes a rather linear pattern. Of course, the pattern is not completely linear, but that is an impossible task with real data. 

The second assumption concerns the independence of residuals. However, as the data is cross-sectional instead of longitudinal, this should not be a problem.  

 The third assumption requires normal distribution of residuals. The normal Q-Q plot examines this assumption. There are no problems if the dots are beautifully following the straight line from corner to corner. However, if the dots (=residuals) diverge, then this assumption might not hold. The Q-Q plot shows that the assumption holds quite well. However, there is some divergence at the top and the bottom of the plot. The tails are “lighter”: the values in both upper and lower bounds are smaller than expected. 
 
 
The fourth and last assumption asks for an equal variance of residuals. There should be approximately. The plot depicting residuals vs. fitted values examines this assumption. The distance of the observations from the fitted line should be the same throughout the whole plot. However, in this case, it varies to some extent. For example, the y-axis goes to -20 on the negative side but only to 5 on the positive side. Nevertheless, if the data is looked at from left to right, there is no “funneling” or curvature effect to be found.   

The last plot, residuals vs. leverage, deals with outliers. Put simply, the straighter the red line is, the less problem the outliers are. Here we are dealing with some outliers, as the line is not straight: first it curves slightly upwards and then downwards. 

According to the diagnostics, some assumptions hold better than others. However, this is expected when modelling data depicting the real world. The model follows most of the assumptions rather well and divergences were not catastrophic. I would say that the model is usable, but the weaknesses should be openly discussed if used. 
